{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "from user_agent import generate_user_agent, generate_navigator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем подмену user agent, выключаем загрузку картинок\n",
    "ua = generate_user_agent(os=('mac', 'linux'))\n",
    "opts = Options()\n",
    "opts.add_argument(\"user-agent={}\".format(ua))\n",
    "\n",
    "chrome_prefs = {}\n",
    "opts.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_prefs[\"profile.managed_default_content_settings\"] = {\"images\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запасные сайты с proxy:\n",
    "# https://openproxy.space/list\n",
    "# https://www.sslproxies.org\n",
    "# http://www.freeproxylists.net/ru/\n",
    "# https://www.proxynova.com/proxy-server-list\n",
    "\n",
    "# Каждая функция собирает proxy с соответствующего сайта и возвращает лист\n",
    "\n",
    "def get_proxy_spysone():\n",
    "    try:\n",
    "        url = 'http://spys.one/en/'\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        driver.get(url)\n",
    "        tags = driver.find_elements_by_class_name('spy14')\n",
    "        proxies = []\n",
    "        for tag in tags:\n",
    "            if re.search('\\d+\\.\\d+\\.\\d+\\.', tag.text):\n",
    "                proxies.append(tag.text)\n",
    "        driver.close()\n",
    "        driver.quit()\n",
    "        return proxies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# похоже этот сайт умер.....\n",
    "def get_proxies_freeproxycz():\n",
    "    try:\n",
    "        url = 'http://free-proxy.cz/en/proxylist/main/1'\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        proxies = []\n",
    "        for i in range(1, 6):\n",
    "            driver.get(url[:-1] + str(i))\n",
    "            td_tags = driver.find_elements_by_tag_name('td')\n",
    "            for td_tag in td_tags:\n",
    "                if re.search('\\d+\\.\\d+\\.\\d+\\.', td_tag.text):\n",
    "                    proxies.append(td_tag.text)\n",
    "        driver.quit()\n",
    "        return proxies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_proxies_proxyscrapecom():\n",
    "    try:\n",
    "        url = 'https://proxyscrape.com/free-proxy-list'\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        driver.get(url)\n",
    "        time.sleep(10)\n",
    "        td_tags = driver.find_element_by_id('proxytable').find_elements_by_tag_name('td')\n",
    "        proxies = []\n",
    "        for td_tag in td_tags:\n",
    "            if re.search('\\d+\\.\\d+\\.\\d+\\.', td_tag.text):\n",
    "                    proxies.append(td_tag.text)\n",
    "        driver.close()\n",
    "        return proxies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_proxies_proxylistdownload():\n",
    "    try:\n",
    "        url = 'https://www.proxy-list.download/HTTP'\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        driver.find_element_by_id('btn3').click()\n",
    "        alert = driver.switch_to.alert\n",
    "        alert.accept()\n",
    "        proxies = pyperclip.paste().split('\\n')\n",
    "        driver.close()\n",
    "        return proxies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_proxies_ipadresscom():\n",
    "    try:\n",
    "        url = 'https://www.ip-adress.com/proxy-list'\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        td_tags = soup.find('tbody').find_all('td')\n",
    "        proxies = [td.text for td in td_tags if td.find('a') != None]\n",
    "        return proxies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_proxies_hidemyname():\n",
    "    try:\n",
    "        url = 'https://hidemy.name/ru/proxy-list/?type=h&start=1#list'\n",
    "        pages = [1, 64]\n",
    "        proxy = []\n",
    "        for p in pages:\n",
    "            string = proxy_url2\n",
    "            url = string[:-6] + str(p) + string[-5:]\n",
    "            driver = webdriver.Chrome(options=opts)\n",
    "            driver.get(url)\n",
    "            time.sleep(10)\n",
    "            for tr in driver.find_elements_by_tag_name('tr')[1:]:\n",
    "                proxy = tr.find_elements_by_tag_name('td')[0].text + ':' + \\\n",
    "                tr.find_elements_by_tag_name('td')[1].text\n",
    "                proxies.append(proxy)\n",
    "            driver.close()\n",
    "        return proxies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# подаем функции выше на вход - на выходе объединенный лист \"сырых\" proxy \n",
    "def create_raw_proxylist(*args):\n",
    "    raw_proxy_list = []\n",
    "    for func in args:\n",
    "        raw_proxy_list += func()\n",
    "    return raw_proxy_list\n",
    "\n",
    "# бьем лист \"сырых\" proxy на куски (нужно для multi-threading)\n",
    "def make_proxylist_chunks(raw_proxy_list, chunk_size):\n",
    "    list_len = len(raw_proxy_list)\n",
    "    chunks = []\n",
    "    for i in range(0, list_len, chunk_size):\n",
    "        chunk = raw_proxy_list[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# фильтруем proxy из \"куска сырого\" proxy-листа на работоспособность - ходим на mail.ru (любой сайт можно впилить) \n",
    "# и добавлем в чистый список, что ошибок не будет. Возвращаем чистый список\n",
    "def filter_raw_proxylist(chunk):\n",
    "    check_url = 'https://auto.ru'\n",
    "    filtered_proxies_chunk = []\n",
    "    for proxy in chunk:\n",
    "        try:\n",
    "            r = requests.get(check_url, \n",
    "                            headers = {'User-Agent':generate_user_agent(os=('mac', 'linux')),\n",
    "                                      'Referer': 'https://mail.ru/'}, \n",
    "                            proxies = {'http': 'http://'+ proxy, 'https':'http://' + proxy},\n",
    "                            timeout=12)\n",
    "            r.encoding = r.apparent_encoding\n",
    "            soup = BeautifulSoup(r.text)\n",
    "# если ходить на auto.ru - нужно проверять на предмет выдачи страницы про перс данные\n",
    "#             try:\n",
    "#                 if 'обработка моих данных' in soup.find('div', class_='text').text:\n",
    "#                     pass    \n",
    "#             except:\n",
    "            filtered_proxies_chunk.append(proxy)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            pass\n",
    "    return filtered_proxies_chunk\n",
    "\n",
    "# функция для выбора рандомного proxy из листа/куска\n",
    "def get_random_proxy(proxies):\n",
    "    return random.choice(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем html\n",
    "def get_soup(url, proxies):\n",
    "    proxy = get_random_proxy(proxies)\n",
    "    r = requests.get(url, \n",
    "                     headers = {'User-Agent':generate_user_agent(os=('mac', 'linux'))}, \n",
    "                     proxies = {'http': 'http://'+proxy, \n",
    "                                'https':'http://'+proxy},\n",
    "                     timeout=12)\n",
    "    r.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "# получаем html на proxy из листа. Если ошибка - удаляем proxy из листа и рекурсивно повторяем, пока лист\n",
    "# proxy не опустошится. Вообще изначально на этом функция выдавала None - но пока поставил рискованное решение - \n",
    "# попробовать в случае опустошения proxy листа зайти без proxy. На 100 000 url сработало, но это, наверно, плохой\n",
    "# подход\n",
    "def get_html(url, proxies):\n",
    "    try:\n",
    "        html = get_soup(url, proxies)\n",
    "        return html\n",
    "    except:\n",
    "        proxy = get_random_proxy(proxies)\n",
    "        proxies.remove(proxy)\n",
    "#         print(f'Удален proxy {proxy}, осталось {len(proxies)}')\n",
    "        if proxies == []:\n",
    "            print('Все proxy битые')\n",
    "            r = requests.get(url)\n",
    "            return BeautifulSoup(r.text, 'html.parser')\n",
    "        html = get_html(url, proxies)\n",
    "        return html\n",
    "\n",
    "# делаем куски из листа ссылок на объявления о продаже\n",
    "def make_links_chunks(links_list, chunk_size):\n",
    "    list_len = len(links_list)\n",
    "    chunks = []\n",
    "    for i in range(0, list_len, chunk_size):\n",
    "        chunk = links_list[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем данные с блока \"card_info\" слева от фотографий\n",
    "def get_main_info(info_dict, offer_soup):\n",
    "    card_info = offer_soup.find('ul', class_='CardInfo')\n",
    "    card_info_classes = [\n",
    "    \"CardInfo__row_year\",\n",
    "    \"CardInfo__row_kmAge\",\n",
    "    \"CardInfo__row_bodytype\",\n",
    "    \"CardInfo__row_color\",\n",
    "    \"CardInfo__row_engine\",\n",
    "    \"CardInfo__row_transportTax\",\n",
    "    \"CardInfo__row_transmission\",\n",
    "    \"CardInfo__row_drive\",\n",
    "    \"CardInfo__row_wheel\",\n",
    "    \"CardInfo__row_state\",\n",
    "    \"CardInfo__row_ownersCount\",\n",
    "    \"CardInfo__row_pts\",\n",
    "    \"CardInfo__row_owningTime\",\n",
    "    \"CardInfo__row_customs\",\n",
    "    \"CardInfo__row_vin\",\n",
    "    \"CardInfo__row_licensePlate\",\n",
    "    ]\n",
    "    \n",
    "    # цикл сбора \n",
    "    for class_ in card_info_classes:\n",
    "        param_name = class_.split('_')[-1]\n",
    "        try:\n",
    "            li_tag = card_info.find('li', class_=class_)\n",
    "            if li_tag == None:\n",
    "                info_dict.update({param_name: []})\n",
    "                info_dict[param_name].append(None)\n",
    "                continue\n",
    "            param_value = li_tag.find_all('span')[-1].text\n",
    "            if param_name not in info_dict.keys():\n",
    "                info_dict.update({param_name: []})\n",
    "                info_dict[param_name].append(param_value)\n",
    "        except:\n",
    "            info_dict[param_name].append(None)\n",
    "\n",
    "# рудимент прошлых попыток - оставил для одного сборщика. Идея - везде в функции пихать try/except неправильно,\n",
    "# поэтому сделал обертку. Но есть функция, ниже (get_reviews_features), которая в эту обертку не залезает - но\n",
    "# ошибку нужно обходить. Поэтому оставил, но вообще эту функцию при оптимизации лучше убрать.\n",
    "def get_info_safely(func):\n",
    "    def func_wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "        except Exception as err:\n",
    "            print(func.__name__)\n",
    "            print(err)\n",
    "            return None\n",
    "        return result\n",
    "    return func_wrapper\n",
    "\n",
    "# берем название модели\n",
    "def get_model_name(info_dict, offer_soup):\n",
    "    try:\n",
    "        price = offer_soup.find('div', class_=\"CardSidebarActions__title\").text\n",
    "    except:\n",
    "        price = None\n",
    "    key='model_name'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})   \n",
    "    info_dict[key].append(price)\n",
    "    \n",
    "# забираем цену\n",
    "def get_price_value(info_dict, offer_soup):\n",
    "    try:\n",
    "        price = offer_soup.find('span', class_='OfferPriceCaption__price').text\n",
    "    except:\n",
    "        price = None\n",
    "    key='price'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})   \n",
    "    info_dict[key].append(price)\n",
    "\n",
    "# забираем изменение цены (далее не используется, так как очень у малого кол-ва объявлений это поле непустое)\n",
    "def get_price_change(info_dict, offer_soup):\n",
    "    try:\n",
    "        price_change = offer_soup.find('div', class_='OfferPriceDiff__diff').text\n",
    "    except:\n",
    "        price_change = None\n",
    "    key  ='price_change'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(price_change)\n",
    "\n",
    "# берем дату публикации\n",
    "def get_publication_date(info_dict, offer_soup):\n",
    "    try:\n",
    "        div_tags = offer_soup.find_all('div')\n",
    "        publication_date = [div for div in div_tags if div.has_attr('title')==True][0].text\n",
    "    except:\n",
    "        publication_date = None\n",
    "    key = 'publication_date'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(publication_date)\n",
    "\n",
    "# берем количество просмотров (далее не используется, так как очень у малого кол-ва объявлений это поле непустое)\n",
    "def get_offer_views(info_dict, offer_soup):\n",
    "    try:\n",
    "        div_tags = offer_soup.find_all('div')\n",
    "        views = [div for div in div_tags if div.has_attr('title')==True][1].text\n",
    "    except:\n",
    "        views = None\n",
    "    key = 'views'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(views)\n",
    "\n",
    "# берем статус продавца как официального дилера  (далее не используется, \n",
    "# так как очень у малого кол-ва объявлений это поле непустое)\n",
    "def get_offdealer_info(info_dict, offer_soup):\n",
    "    try:\n",
    "        offdealer_info = offer_soup.find('div', class_='CardDealerInfo__salonVerifiedLabel').text\n",
    "    except:\n",
    "        offdealer_info = ''\n",
    "    key = 'official_dealer'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(offdealer_info)\n",
    "\n",
    "# берем рейтинги (далее не используется, \n",
    "# так как очень у малого кол-ва объявлений это поле непустое)\n",
    "def get_reviews_rating(info_dict, offer_soup):\n",
    "    regex = re.compile(r'\"avg_rating\":\\d+.\\d+,\"counter\":\\d+')\n",
    "    try:\n",
    "        match = regex.search(str(offer_soup))[0]\n",
    "        reviews_rating = match.split(',')[0].split(':')[1]\n",
    "    except:\n",
    "        reviews_rating = 0\n",
    "    key = 'reviews_avg_rating'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(reviews_rating)\n",
    "\n",
    "# берем количество обзоров (далее не используется, \n",
    "# так как очень у малого кол-ва объявлений это поле непустое)\n",
    "def get_reviews_count(info_dict, offer_soup):\n",
    "    regex = re.compile(r'\"avg_rating\":\\d+.\\d+,\"counter\":\\d+')\n",
    "    try:\n",
    "        match = regex.search(str(offer_soup))[0] \n",
    "        reviews_count = match.split(',')[1].split(':')[1]\n",
    "    except:\n",
    "        reviews_count = 0\n",
    "    key = 'reviews_count'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(reviews_count)\n",
    "\n",
    "# берем оценки параметров модели (далее не используется, \n",
    "# так как очень у малого кол-ва объявлений это поле непустое)\n",
    "@get_info_safely    \n",
    "def get_reviews_features(info_dict, offer_soup):\n",
    "    regex = re.compile(r'\"reviewsFeatures\":{.*{.*{.*\\[{.*}]}}}')\n",
    "    string = '{' + regex.search(str(offer_soup))[0] + '}'\n",
    "    reviews_dict = json.loads(string, encoding='utf-8')\n",
    "    keys = reviews_dict['reviewsFeatures']['data']['features'].keys()\n",
    "    for key in keys:\n",
    "        temp_dict = reviews_dict['reviewsFeatures']['data']['features'][key]\n",
    "        for item in temp_dict:\n",
    "            name_pos = item['type'] + '_pos'\n",
    "            name_neg = item['type'] + '_neg'\n",
    "            name_ratio = item['type'] + '_ratio'\n",
    "            val_pos = item['plus_count']\n",
    "            val_neg = item['minus_count']\n",
    "            val_ratio = item['plus_count']/item['total_count']\n",
    "            for dict_key in (name_pos, name_neg, name_ratio):\n",
    "                if dict_key not in info_dict.keys():\n",
    "                    dict_val_len = len(info_dict['year'])\n",
    "                    info_dict.update({dict_key: ['']*(info_dict_len-1)})\n",
    "            info_dict[name_pos].append(val_pos)\n",
    "            info_dict[name_neg].append(val_neg)\n",
    "            info_dict[name_ratio].append(val_ratio)\n",
    "\n",
    "# подсчитываем количество фотографий\n",
    "def get_photo_count(info_dict, offer_soup):\n",
    "    try:\n",
    "        photo_count = len(offer_soup.find('div', class_=\"ImageGalleryDesktop__thumbs-container\").\n",
    "                          find_all('div'))\n",
    "    except:\n",
    "        photo_count = 0\n",
    "    key = 'photo_count'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(photo_count)\n",
    "\n",
    "# подсчитываем количество элементов комплектации (далее не используется, \n",
    "# так как очень у малого кол-ва объявлений это поле непустое)\n",
    "@get_info_safely\n",
    "def get_complectation_count(info_dict, offer_soup):\n",
    "    div_tags = offer_soup.findAll('div', class_=\"CardComplectation__group\")\n",
    "    complectation_names = [div.find('span', class_=\"CardComplectation__itemName\").text \n",
    "                           for div in div_tags]\n",
    "    complectation_counts = [int(div.find('span', class_='CardComplectation__itemCount').text)\n",
    "                           for div in div_tags]\n",
    "    \n",
    "    for complectation_name, complectation_count in zip(complectation_names,\n",
    "                                                        complectation_counts):\n",
    "        key = 'Комплектация_' + complectation_name\n",
    "        if key not in info_dict.keys():\n",
    "            info_dict.update({dict_key: ['']*(info_dict_len-1)})\n",
    "        info_dict[key].append(complectation_count)\n",
    "\n",
    "# ищем информацию о том, что машина на гарантии\n",
    "def get_warranty_info(info_dict, offer_soup):\n",
    "    try:\n",
    "        descr = offer_soup.find('div', class_=\"CardDescription__textInner\").text\n",
    "    except:\n",
    "        descr = ' '\n",
    "    regex = re.compile('гаранти[ия]')\n",
    "    key = 'on_warranty'\n",
    "    warranty = int((re.search(regex, descr.lower()) != None))\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({dict_key: ['']*(info_dict_len-1)})\n",
    "    info_dict[key].append(warranty)\n",
    "\n",
    "# ищем информацию о дтп\n",
    "def get_crashes_info(info_dict, offer_soup):\n",
    "    try:\n",
    "        div_tags = offer_soup.find_all('div', class_=\"CardBenefits__item-title\")\n",
    "        no_crashes = 0\n",
    "        for div in div_tags:\n",
    "            if 'ДТП не найдены' in div.text:\n",
    "                no_crashes = 1\n",
    "    except:\n",
    "        no_crashes = 0\n",
    "    key = 'no_crashes'\n",
    "    if key not in info_dict.keys():\n",
    "        info_dict.update({key: []})\n",
    "    info_dict[key].append(no_crashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем ссылки на объявления, скачанные в отдельные файлы на прошлом этапе в DataFrame\n",
    "data = pd.DataFrame()\n",
    "for file in os.listdir('../../../Desktop/Autoru/'):\n",
    "    if file.endswith('.xlsx'):\n",
    "        df = pd.read_excel('~/Desktop/Autoru/' + file, headers=None)\n",
    "        data = pd.concat([data, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем куски по 150 ссылок для мульти-трединга\n",
    "links_list = list(data[0].values)\n",
    "links_chunks = make_links_chunks(links_list, chunk_size=150)\n",
    "print(len(links_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь, в который будут падать данные. Структура {'url объявления': {'параметр': значение}}\n",
    "# сделано так, чтобы мульти-трединг работал по каждой ссылке отдельно - иначе одни и те же\n",
    "# параметры и значения могут затираться тем или иным тредом\n",
    "data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# финальный сборщик данных\n",
    "def get_info(url, data_dict, proxies):  \n",
    "    data_dict.update({url:{}})\n",
    "    info_dict = data_dict[url]\n",
    "    offer_soup = get_html(url, proxies)\n",
    "    get_main_info(info_dict, offer_soup)\n",
    "    get_model_name(info_dict, offer_soup)\n",
    "    get_price_value(info_dict, offer_soup)\n",
    "    get_price_change(info_dict, offer_soup)\n",
    "    get_publication_date(info_dict, offer_soup)\n",
    "    get_offer_views(info_dict, offer_soup)\n",
    "    get_photo_count(info_dict, offer_soup)\n",
    "    get_offdealer_info(info_dict, offer_soup)\n",
    "#     get_complectation_count(info_dict, offer_soup)\n",
    "    get_warranty_info(info_dict, offer_soup)\n",
    "    get_crashes_info(info_dict, offer_soup)\n",
    "#     get_reviews_rating(info_dict, offer_soup)\n",
    "#     get_reviews_count(info_dict, offer_soup)\n",
    "#     get_reviews_features(info_dict, offer_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем \"сырой\" proxy-лист (можно убрать этот блок, делалось, чтобы следующую ячейку можно было тестировать,\n",
    "# не запуская каждый раз сборку чистого proxy-листа - это делается несколько минут)\n",
    "raw_proxy_list = create_raw_proxylist(get_proxy_spysone, get_proxies_freeproxycz,\n",
    "                         get_proxies_proxyscrapecom, get_proxies_proxylistdownload,\n",
    "                         get_proxies_ipadresscom, get_proxies_hidemyname)\n",
    "# размер куска для мульти-тред проверки\n",
    "chunk_size = 10\n",
    "# делаем куски \"сырого\" листа\n",
    "chunks = make_proxylist_chunks(raw_proxy_list, chunk_size)\n",
    "\n",
    "# проверяем все куски в отдельном треде (около 100 тредов одновременно)\n",
    "proxies = []\n",
    "for _ in range(4):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as ex:\n",
    "        filtered_proxies_chunks = ex.map(filter_raw_proxylist, chunks)\n",
    "        for filtered_chunk in filtered_proxies_chunks:\n",
    "            proxies += filtered_chunk\n",
    "print(len(proxies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основной блок\n",
    "len_proxies = len(proxies)\n",
    "\n",
    "# для каждого куска листа ссылок на объявления\n",
    "for i, chunk in tqdm(enumerate(links_chunks)):\n",
    "# если что-то сломалось на каком-то шаге - можно пропустить выполненные\n",
    "#     if i < 14:\n",
    "#         continue\n",
    "\n",
    "# если чистых proxy меньше 100 штук в листе (обычно столько хватает на обработку 150 ссылок 150ю тредами)-\n",
    "# запускам сбор proxy листа\n",
    "    if len_proxies < 100:\n",
    "        raw_proxy_list = create_raw_proxylist(get_proxy_spysone, get_proxies_freeproxycz,\n",
    "                                 get_proxies_proxyscrapecom, get_proxies_proxylistdownload,\n",
    "                                 get_proxies_ipadresscom, get_proxies_hidemyname)\n",
    "        chunk_size = 10\n",
    "        chunks = make_proxylist_chunks(raw_proxy_list, chunk_size)\n",
    "# нужно 4 запуска, чтобы собрать 100 полу-живых proxy\n",
    "        for _ in range(4):\n",
    "            with concurrent.futures.ThreadPoolExecutor() as ex:\n",
    "                filtered_proxies_chunks = ex.map(filter_raw_proxylist, chunks)\n",
    "                for filtered_chunk in filtered_proxies_chunks:\n",
    "                    proxies += filtered_chunk\n",
    "    print(f'Len proxies = {len_proxies}')\n",
    "\n",
    "# основной блок сбора информации - берем кусок листа ссылок (chunk), и, пользуюясь чистым proxy-листом\n",
    "# собираем данные по объявлениям\n",
    "# этот цикл работает около 50 часов\n",
    "    with concurrent.futures.ThreadPoolExecutor() as ex:\n",
    "        ex.map(partial(get_info, data_dict=data_dict, proxies = proxies), chunk)\n",
    "# смотрим, сколько \"живых\" proxy осталось\n",
    "    len_proxies = len(proxies)\n",
    "# каждые 5*len(chunk) обработанных объявлений (750 в данном случае, изначально было 1000, но не хватало proxy)\n",
    "# сохраняем info_dict в json на жесткий диск (накопительный файл, последний содержит всю предыдущую информацию)\n",
    "    if (i+1) % 5 == 0:\n",
    "        number = str(i+1)\n",
    "        with open('../../../Desktop/data/data_' + str(i) + '.json', 'w', encoding='mac_cyrillic') as file:\n",
    "            json.dump(data_dict, file)\n",
    "    print(f'{i+1} chunk finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
